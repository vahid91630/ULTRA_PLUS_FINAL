#!/usr/bin/env python3
"""
ğŸ¤– Automated Daily Learning Processor
Fully automated pipeline that runs daily at 23:59 UTC
Processes, scores, stores, and reports learning insights
"""

import asyncio
import json
import logging
import os
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from pymongo import MongoClient
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import telegram
from telegram.error import TelegramError

logger = logging.getLogger(__name__)

class AutomatedDailyProcessor:
    """
    Fully automated daily learning processor with scheduling
    """
    
    def __init__(self, telegram_token: str = None, chat_id: str = None, mongodb_uri: str = None):
        # Configuration
        self.telegram_token = telegram_token or os.getenv('TELEGRAM_BOT_TOKEN')
        self.chat_id = chat_id or os.getenv('TELEGRAM_CHAT_ID', '559649958')  # Vahid's chat ID
        self.mongodb_uri = mongodb_uri or os.getenv('DATABASE_URL')
        
        # Components
        self.scheduler = BackgroundScheduler(timezone='UTC')
        self.bot = None
        self.is_running = False
        
        # Scoring configuration
        self.scoring_config = {
            'freshness_points': 20,     # <48 hours
            'trusted_source_points': 30, # Trusted sources
            'relevant_tag_points': 10,   # Per relevant tag
            'minimum_score': 60         # Filter threshold
        }
        
        # Trusted sources (optimized for memory)
        self.trusted_sources = frozenset([
            'investopedia', 'coingecko', 'binance', 'tradingview',
            'coindesk', 'cointelegraph', 'marketwatch', 'reuters',
            'bloomberg', 'yahoo finance'
        ])
        
        # Key financial tags (optimized for memory)
        self.key_tags = frozenset([
            'rsi', 'btc', 'eth', 'bitcoin', 'ethereum', 'breakout',
            'support', 'resistance', 'trend', 'volume', 'trading',
            'macd', 'bollinger', 'analysis', 'bullish', 'bearish'
        ])
        
        logger.info("ğŸ¤– Automated Daily Processor initialized")
    
    def start_automation(self):
        """Start the automated daily processing"""
        if self.is_running:
            logger.warning("Automation already running")
            return
        
        try:
            # Initialize Telegram bot
            if self.telegram_token:
                self.bot = telegram.Bot(token=self.telegram_token)
                logger.info("âœ… Telegram bot initialized")
            else:
                logger.warning("âš ï¸ No Telegram token provided")
            
            # Schedule daily processing at 23:59 UTC
            self.scheduler.add_job(
                func=self._run_daily_processing,
                trigger=CronTrigger(hour=23, minute=59, timezone='UTC'),
                id='daily_learning_processor',
                name='Daily Learning Processor',
                replace_existing=True,
                max_instances=1
            )
            
            # Add test job (runs every 5 minutes for testing)
            # Remove this in production
            self.scheduler.add_job(
                func=self._run_test_processing,
                trigger=CronTrigger(minute='*/5'),
                id='test_processor',
                name='Test Processor (5min)',
                replace_existing=True,
                max_instances=1
            )
            
            # Start scheduler
            self.scheduler.start()
            self.is_running = True
            
            logger.info("ğŸš€ Automated daily processing started")
            logger.info("ğŸ“… Scheduled for 23:59 UTC daily")
            
        except Exception as e:
            logger.error(f"âŒ Failed to start automation: {e}")
            raise
    
    def stop_automation(self):
        """Stop the automated processing"""
        if self.scheduler.running:
            self.scheduler.shutdown(wait=False)
        self.is_running = False
        logger.info("ğŸ›‘ Automated processing stopped")
    
    def _run_daily_processing(self):
        """Run the daily processing pipeline"""
        logger.info("ğŸŒ™ Starting automated daily processing at 23:59 UTC")
        
        try:
            # Create new event loop for this thread
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # Run the async processing
            result = loop.run_until_complete(self._process_daily_pipeline())
            
            logger.info(f"âœ… Daily processing completed: {result}")
            
        except Exception as e:
            logger.error(f"âŒ Daily processing failed: {e}")
        finally:
            loop.close()
    
    def _run_test_processing(self):
        """Run test processing (remove in production)"""
        logger.info("ğŸ§ª Running test processing cycle")
        
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(self._process_daily_pipeline())
            logger.info(f"ğŸ§ª Test processing result: {result}")
        except Exception as e:
            logger.error(f"âŒ Test processing failed: {e}")
        finally:
            loop.close()
    
    async def _process_daily_pipeline(self) -> Dict[str, Any]:
        """Execute the complete daily processing pipeline"""
        start_time = datetime.now()
        results = {
            'timestamp': start_time.isoformat(),
            'success': False,
            'total_entries': 0,
            'qualified_entries': 0,
            'stored_entries': 0,
            'top_insight': None,
            'errors': []
        }
        
        try:
            # Step 1: Read daily data
            logger.info("ğŸ“– Step 1: Reading daily data")
            raw_data = await self._read_daily_data()
            results['total_entries'] = len(raw_data)
            
            if not raw_data:
                results['errors'].append("No daily data found")
                await self._send_telegram_report(results)
                return results
            
            # Step 2: Score and filter entries
            logger.info("ğŸ§® Step 2: Scoring and filtering entries")
            qualified_entries = await self._score_and_filter_entries(raw_data)
            results['qualified_entries'] = len(qualified_entries)
            
            if not qualified_entries:
                results['errors'].append("No entries met minimum score")
                await self._send_telegram_report(results)
                return results
            
            # Step 3: Store to MongoDB
            logger.info("ğŸ’¾ Step 3: Storing to MongoDB")
            stored_count = await self._store_to_mongodb(qualified_entries)
            results['stored_entries'] = stored_count
            
            # Step 4: Update knowledge base
            logger.info("ğŸ§  Step 4: Updating knowledge base")
            await self._update_knowledge_base(qualified_entries)
            
            # Step 5: Find top insight
            if qualified_entries:
                results['top_insight'] = max(qualified_entries, key=lambda x: x.get('score', 0))
            
            # Step 6: Cleanup local data
            logger.info("ğŸ§¹ Step 5: Cleaning up local data")
            await self._cleanup_local_data()
            
            results['success'] = True
            results['duration'] = (datetime.now() - start_time).total_seconds()
            
            # Step 7: Send Telegram report
            await self._send_telegram_report(results)
            
            logger.info("âœ… Daily pipeline completed successfully")
            
        except Exception as e:
            error_msg = f"Pipeline error: {str(e)}"
            results['errors'].append(error_msg)
            logger.error(f"âŒ {error_msg}")
            await self._send_telegram_report(results)
        
        return results
    
    async def _read_daily_data(self) -> List[Dict]:
        """Read daily data from local sources"""
        data = []
        
        # Try JSON file first
        json_files = ['daily_data.json', 'collected_data.json', 'learning_data.json']
        for json_file in json_files:
            if os.path.exists(json_file):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        file_data = json.load(f)
                        if isinstance(file_data, list):
                            data.extend(file_data)
                        elif isinstance(file_data, dict) and 'entries' in file_data:
                            data.extend(file_data['entries'])
                    logger.info(f"ğŸ“– Read {len(data)} entries from {json_file}")
                    break
                except Exception as e:
                    logger.error(f"Error reading {json_file}: {e}")
        
        # Fallback to Replit DB
        if not data and 'REPL_ID' in os.environ:
            try:
                from replit import db
                data = db.get('daily_entries', [])
                logger.info(f"ğŸ“– Read {len(data)} entries from Replit DB")
            except ImportError:
                logger.warning("Replit DB not available")
        
        # Create sample data for testing if nothing found
        if not data:
            data = self._create_sample_data()
            logger.info(f"ğŸ“ Created {len(data)} sample entries for testing")
        
        return data
    
    def _create_sample_data(self) -> List[Dict]:
        """Create realistic sample data for testing"""
        now = datetime.now()
        return [
            {
                'content': 'Bitcoin breaks key resistance at $68,000 with massive volume surge',
                'timestamp': now.isoformat(),
                'source': 'CoinGecko',
                'tags': ['BTC', 'breakout', 'volume', 'resistance']
            },
            {
                'content': 'Ethereum shows strong RSI divergence signaling potential reversal',
                'timestamp': (now - timedelta(hours=6)).isoformat(),
                'source': 'TradingView',
                'tags': ['ETH', 'RSI', 'bullish', 'analysis']
            },
            {
                'content': 'Understanding support and resistance in cryptocurrency trading',
                'timestamp': (now - timedelta(hours=12)).isoformat(),
                'source': 'Investopedia',
                'tags': ['support', 'resistance', 'trading', 'education']
            }
        ]
    
    async def _score_and_filter_entries(self, entries: List[Dict]) -> List[Dict]:
        """Score entries and filter by minimum threshold"""
        qualified_entries = []
        
        for entry in entries:
            score = self._calculate_score(entry)
            
            if score >= self.scoring_config['minimum_score']:
                entry['score'] = score
                entry['processed_timestamp'] = datetime.now().isoformat()
                qualified_entries.append(entry)
                
                logger.debug(f"âœ… Qualified (score: {score}): {entry.get('content', '')[:50]}...")
            else:
                logger.debug(f"âŒ Rejected (score: {score}): {entry.get('content', '')[:50]}...")
        
        # Sort by score descending
        qualified_entries.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        logger.info(f"ğŸ“Š Qualified {len(qualified_entries)} entries from {len(entries)} total")
        return qualified_entries
    
    def _calculate_score(self, entry: Dict) -> int:
        """Calculate score for a single entry"""
        score = 0
        
        try:
            # Freshness scoring (+20 if <48 hours)
            timestamp_str = entry.get('timestamp', '')
            if timestamp_str:
                try:
                    entry_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                    age_hours = (datetime.now() - entry_time.replace(tzinfo=None)).total_seconds() / 3600
                    
                    if age_hours < 48:
                        score += self.scoring_config['freshness_points']
                except (ValueError, TypeError):
                    pass
            
            # Trusted source scoring (+30)
            source = entry.get('source', '').lower().strip()
            if any(trusted in source for trusted in self.trusted_sources):
                score += self.scoring_config['trusted_source_points']
            
            # Relevant tags scoring (+10 per tag)
            tags = entry.get('tags', [])
            if isinstance(tags, list):
                relevant_count = sum(1 for tag in tags 
                                   if str(tag).lower().strip() in self.key_tags)
                score += relevant_count * self.scoring_config['relevant_tag_points']
            
        except Exception as e:
            logger.error(f"Scoring error: {e}")
        
        return score
    
    async def _store_to_mongodb(self, qualified_entries: List[Dict]) -> int:
        """Store qualified entries to MongoDB"""
        if not self.mongodb_uri or not qualified_entries:
            return 0
        
        client = None
        try:
            # Connect with minimal settings for free tier
            client = MongoClient(
                self.mongodb_uri,
                maxPoolSize=3,
                serverSelectionTimeoutMS=5000,
                connectTimeoutMS=5000
            )
            
            db = client.trading_bot
            collection = db.daily_insights
            
            # Delete previous day's entries to save space
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            delete_result = collection.delete_many({
                'date_saved': {'$regex': f'^{yesterday}'}
            })
            logger.info(f"ğŸ—‘ï¸ Deleted {delete_result.deleted_count} old entries")
            
            # Prepare documents
            documents = []
            current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            for entry in qualified_entries:
                doc = {
                    'content': str(entry.get('content', ''))[:500],  # Limit size
                    'score': entry.get('score', 0),
                    'tags': (entry.get('tags') or [])[:8],  # Limit tags
                    'source': str(entry.get('source', '')),
                    'date_saved': current_date,
                    'original_timestamp': entry.get('timestamp', ''),
                    'processed_by': 'automated_daily_processor'
                }
                documents.append(doc)
            
            # Insert in batches
            batch_size = 20
            inserted_total = 0
            
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                result = collection.insert_many(batch)
                inserted_total += len(result.inserted_ids)
            
            logger.info(f"ğŸ’¾ Stored {inserted_total} entries to MongoDB")
            return inserted_total
            
        except Exception as e:
            logger.error(f"MongoDB storage error: {e}")
            return 0
        finally:
            if client:
                client.close()
    
    async def _update_knowledge_base(self, qualified_entries: List[Dict]):
        """Update the bot's knowledge base with high-quality insights"""
        if not self.mongodb_uri:
            return
        
        # Only save top-scoring entries (score >= 80) to knowledge base
        high_quality_entries = [e for e in qualified_entries if e.get('score', 0) >= 80]
        
        if not high_quality_entries:
            logger.info("ğŸ“š No high-quality entries for knowledge base")
            return
        
        client = None
        try:
            client = MongoClient(self.mongodb_uri, maxPoolSize=2, serverSelectionTimeoutMS=3000)
            knowledge_collection = client.trading_bot.knowledge
            
            # Prepare knowledge documents
            knowledge_docs = []
            for entry in high_quality_entries:
                doc = {
                    'insight': entry.get('content', '')[:300],
                    'source': entry.get('source', ''),
                    'tags': entry.get('tags', [])[:5],
                    'confidence_score': entry.get('score', 0) / 100,  # Normalize to 0-1
                    'added_date': datetime.now().isoformat(),
                    'category': 'daily_learning'
                }
                knowledge_docs.append(doc)
            
            # Insert knowledge
            if knowledge_docs:
                result = knowledge_collection.insert_many(knowledge_docs)
                logger.info(f"ğŸ§  Added {len(result.inserted_ids)} insights to knowledge base")
                
                # Keep knowledge base size manageable (max 1000 entries)
                total_count = knowledge_collection.estimated_document_count()
                if total_count > 1000:
                    # Remove oldest entries
                    oldest_entries = knowledge_collection.find().sort('added_date', 1).limit(total_count - 1000)
                    old_ids = [doc['_id'] for doc in oldest_entries]
                    knowledge_collection.delete_many({'_id': {'$in': old_ids}})
                    logger.info(f"ğŸ§¹ Cleaned old knowledge entries")
            
        except Exception as e:
            logger.error(f"Knowledge base update error: {e}")
        finally:
            if client:
                client.close()
    
    async def _cleanup_local_data(self):
        """Clean up local data files after successful processing"""
        try:
            files_to_clean = ['daily_data.json', 'collected_data.json', 'learning_data.json']
            
            for filename in files_to_clean:
                if os.path.exists(filename):
                    # Create backup with timestamp
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
                    backup_name = f"processed_{filename}_{timestamp}"
                    os.rename(filename, backup_name)
                    logger.info(f"ğŸ“¦ Backed up {filename} to {backup_name}")
            
            # Clean old backups (keep last 3)
            backup_files = [f for f in os.listdir('.') if f.startswith('processed_')]
            if len(backup_files) > 3:
                backup_files.sort(key=os.path.getmtime)
                for old_file in backup_files[:-3]:
                    os.remove(old_file)
                    logger.info(f"ğŸ—‘ï¸ Removed old backup: {old_file}")
            
            # Clear Replit DB
            if 'REPL_ID' in os.environ:
                try:
                    from replit import db
                    if 'daily_entries' in db:
                        del db['daily_entries']
                        logger.info("ğŸ§¹ Cleared Replit DB")
                except ImportError:
                    pass
                    
        except Exception as e:
            logger.error(f"Cleanup error: {e}")
    
    async def _send_telegram_report(self, results: Dict):
        """Send automated report to Telegram"""
        if not self.bot or not self.chat_id:
            logger.warning("No Telegram bot or chat_id configured")
            return
        
        try:
            # Format report message in Persian
            if results['success']:
                top_insight = results.get('top_insight', {})
                top_content = top_insight.get('content', 'Ù†Ø§Ù…Ø´Ø®Øµ')[:100] + '...'
                top_score = top_insight.get('score', 0)
                
                message = f"""
ğŸ¤– **Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯Ú©Ø§Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡**
â° Ø²Ù…Ø§Ù†: {datetime.now().strftime('%Y-%m-%d %H:%M')} UTC

ğŸ“Š **Ù†ØªØ§ÛŒØ¬ Ù¾Ø±Ø¯Ø§Ø²Ø´:**
â€¢ Ù…Ø¬Ù…ÙˆØ¹ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§: {results['total_entries']:,}
â€¢ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ§Ø¬Ø¯ Ø´Ø±Ø§ÛŒØ·: {results['qualified_entries']:,}
â€¢ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± MongoDB: {results['stored_entries']:,}

ğŸ† **Ø¨Ø±ØªØ±ÛŒÙ† Ø¨ÛŒÙ†Ø´ (Ø§Ù…ØªÛŒØ§Ø²: {top_score}):**
{top_content}

âœ… **ÙˆØ¶Ø¹ÛŒØª:** Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯
ğŸ§  **Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´:** Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯ Ø¨Ø§ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ø¹Ø§Ù„ÛŒ

ğŸ“… **Ø¨Ø¹Ø¯ÛŒ:** ÙØ±Ø¯Ø§ Ø¯Ø± Ø³Ø§Ø¹Øª 23:59 UTC
                """.strip()
            else:
                errors = '\n'.join(f"â€¢ {error}" for error in results.get('errors', []))
                message = f"""
ğŸ¤– **Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯Ú©Ø§Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡**
â° Ø²Ù…Ø§Ù†: {datetime.now().strftime('%Y-%m-%d %H:%M')} UTC

âŒ **Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®ÙˆØ¯Ú©Ø§Ø±**

ğŸ“Š **Ø¢Ù…Ø§Ø±:**
â€¢ Ù…Ø¬Ù…ÙˆØ¹ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§: {results['total_entries']:,}
â€¢ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ§Ø¬Ø¯ Ø´Ø±Ø§ÛŒØ·: {results['qualified_entries']:,}

ğŸš¨ **Ù…Ø´Ú©Ù„Ø§Øª:**
{errors}

ğŸ”§ **ØªÙˆØµÛŒÙ‡:** Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ MongoDB Ùˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡
                """.strip()
            
            # Send message
            await self.bot.send_message(
                chat_id=self.chat_id,
                text=message,
                parse_mode='Markdown'
            )
            
            logger.info("ğŸ“± Telegram report sent successfully")
            
        except TelegramError as e:
            logger.error(f"Telegram send error: {e}")
        except Exception as e:
            logger.error(f"Report generation error: {e}")
    
    def get_status(self) -> Dict:
        """Get current automation status"""
        jobs = []
        if self.scheduler.running:
            for job in self.scheduler.get_jobs():
                jobs.append({
                    'id': job.id,
                    'name': job.name,
                    'next_run': job.next_run_time.isoformat() if job.next_run_time else None,
                    'trigger': str(job.trigger)
                })
        
        return {
            'is_running': self.is_running,
            'scheduler_running': self.scheduler.running if hasattr(self, 'scheduler') else False,
            'telegram_configured': bool(self.bot),
            'mongodb_configured': bool(self.mongodb_uri),
            'scheduled_jobs': jobs,
            'last_status_check': datetime.now().isoformat()
        }

# Global processor instance
daily_processor = None

def start_automated_learning(telegram_token: str = None, chat_id: str = None, mongodb_uri: str = None):
    """Start the automated daily learning system"""
    global daily_processor
    
    if daily_processor and daily_processor.is_running:
        logger.warning("Automated learning already running")
        return daily_processor
    
    daily_processor = AutomatedDailyProcessor(
        telegram_token=telegram_token,
        chat_id=chat_id,
        mongodb_uri=mongodb_uri
    )
    
    daily_processor.start_automation()
    return daily_processor

def stop_automated_learning():
    """Stop the automated daily learning system"""
    global daily_processor
    
    if daily_processor:
        daily_processor.stop_automation()
        daily_processor = None

def get_automation_status() -> Dict:
    """Get automation status"""
    global daily_processor
    
    if daily_processor:
        return daily_processor.get_status()
    else:
        return {
            'is_running': False,
            'message': 'Automation not started'
        }

# Integration with existing bot
def integrate_with_telegram_bot(application, mongodb_uri: str = None):
    """Integrate automation with existing Telegram bot"""
    from telegram.ext import CommandHandler
    
    async def start_automation_command(update, context):
        """Command to start automation"""
        try:
            processor = start_automated_learning(
                telegram_token=context.bot.token,
                chat_id=str(update.effective_chat.id),
                mongodb_uri=mongodb_uri
            )
            
            status = processor.get_status()
            next_job = status['scheduled_jobs'][0] if status['scheduled_jobs'] else {}
            next_run = next_job.get('next_run', 'Ù†Ø§Ù…Ø´Ø®Øµ')
            
            response = f"""
âœ… **Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ¹Ø§Ù„ Ø´Ø¯**

â° **Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ:** Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¯Ø± Ø³Ø§Ø¹Øª 23:59 UTC
ğŸ”„ **Ø¨Ø¹Ø¯ÛŒ:** {next_run[:19] if next_run != 'Ù†Ø§Ù…Ø´Ø®Øµ' else 'Ù†Ø§Ù…Ø´Ø®Øµ'}

ğŸ¤– **Ø¹Ù…Ù„ÛŒØ§Øª Ø®ÙˆØ¯Ú©Ø§Ø±:**
â€¢ Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡
â€¢ Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ùˆ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†
â€¢ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± MongoDB
â€¢ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´
â€¢ Ø§Ø±Ø³Ø§Ù„ Ú¯Ø²Ø§Ø±Ø´ ØªÙ„Ú¯Ø±Ø§Ù…

âœ… **ÙˆØ¶Ø¹ÛŒØª:** ÙØ¹Ø§Ù„ Ùˆ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§
            """.strip()
            
            await update.message.reply_text(response, parse_mode='Markdown')
            
        except Exception as e:
            await update.message.reply_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ: {str(e)}")
    
    async def stop_automation_command(update, context):
        """Command to stop automation"""
        try:
            stop_automated_learning()
            await update.message.reply_text("ğŸ›‘ Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
        except Exception as e:
            await update.message.reply_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ‚Ù: {str(e)}")
    
    async def automation_status_command(update, context):
        """Command to check automation status"""
        try:
            status = get_automation_status()
            
            if status['is_running']:
                jobs_info = '\n'.join(f"â€¢ {job['name']}: {job['next_run'][:19] if job['next_run'] else 'Ù†Ø§Ù…Ø´Ø®Øµ'}" 
                                    for job in status.get('scheduled_jobs', []))
                
                response = f"""
ğŸ“Š **ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±**

âœ… **ÙˆØ¶Ø¹ÛŒØª:** ÙØ¹Ø§Ù„
ğŸ”„ **Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯:** {"ÙØ¹Ø§Ù„" if status['scheduler_running'] else "ØºÛŒØ±ÙØ¹Ø§Ù„"}
ğŸ“± **ØªÙ„Ú¯Ø±Ø§Ù…:** {"Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡" if status['telegram_configured'] else "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ"}
ğŸ’¾ **MongoDB:** {"Ù…ØªØµÙ„" if status['mongodb_configured'] else "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§ØªØµØ§Ù„"}

â° **Ú©Ø§Ø±Ù‡Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡:**
{jobs_info}

ğŸ• **Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒ:** {status['last_status_check'][:19]}
                """.strip()
            else:
                response = "âŒ Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ¹Ø§Ù„ Ù†ÛŒØ³Øª\n\nØ¨Ø±Ø§ÛŒ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ø² /start_automation Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯"
            
            await update.message.reply_text(response, parse_mode='Markdown')
            
        except Exception as e:
            await update.message.reply_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª: {str(e)}")
    
    # Add commands
    application.add_handler(CommandHandler("start_automation", start_automation_command))
    application.add_handler(CommandHandler("stop_automation", stop_automation_command))
    application.add_handler(CommandHandler("automation_status", automation_status_command))
    
    logger.info("ğŸ“± Automation commands added to Telegram bot")

# Standalone execution for testing
def main():
    """Main function for standalone testing"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Start automation
    processor = start_automated_learning()
    
    try:
        logger.info("ğŸ¤– Automated learning system started")
        logger.info("Press Ctrl+C to stop")
        
        # Keep running
        import time
        while True:
            status = processor.get_status()
            logger.info(f"Status: {status['is_running']}, Jobs: {len(status['scheduled_jobs'])}")
            time.sleep(60)  # Status update every minute
            
    except KeyboardInterrupt:
        logger.info("Stopping automated learning...")
        stop_automated_learning()
        logger.info("âœ… Stopped")

if __name__ == "__main__":
    main()